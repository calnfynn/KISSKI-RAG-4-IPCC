{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda795eb",
   "metadata": {},
   "source": [
    "- create .venv\n",
    "- run `pip install -r requirements.txt`\n",
    "- get API key from https://kisski.gwdg.de/leistungen/2-02-llm-service/\n",
    "- create `.env` file with:\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://chat-ai.academiccloud.de/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c70a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "index_dir = \"./faiss_index\"\n",
    "tokens_per_chunk = 1024\n",
    "embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "topic = \"climate change and its effects on islands\"\n",
    "\n",
    "class Prompt(Enum):\n",
    "    BASIC = f'You are an expert on the topic of {topic} . You are explaining it to someone with basic knowledge of the topic.'\n",
    "    ADVANCED = f'You are an expert on the topic of {topic}. You are explaining it to someone with advanced knowledge of the topic.'\n",
    "\n",
    "class Model(Enum):\n",
    "    LLAMA = 'meta-llama-3.1-8b-instruct'\n",
    "    MISTRAL = 'Mistral-Large-Instruct-2407'\n",
    "    GEMMA = 'gemma-3-27b-it'\n",
    "\n",
    "llm_model = Model.LLAMA.value\n",
    "answer_level = Prompt.ADVANCED.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4743fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s).\n",
      "Generated 326 chunks.\n",
      "Using stored index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "\n",
    "# Load HTML File/s\n",
    "documents = SimpleDirectoryReader(input_dir=\"html\").load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "# Chunk with SentenceSplitter\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=tokens_per_chunk, chunk_overlap=200) \n",
    "# 1 token = 4 characters\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# # show chunks\n",
    "# for i, node in enumerate(nodes):\n",
    "#     print(f\"Chunk {i}:\\n{node.get_content()}\\n{'='*40}\")\n",
    "    \n",
    "print(f\"Generated {len(nodes)} chunks.\") \n",
    "\n",
    "\n",
    "# Embed Chunks with HuggingFace\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model)\n",
    "\n",
    "if os.path.exists(index_dir) and os.listdir(index_dir):\n",
    "  vector_store = FaissVectorStore.from_persist_dir(index_dir)\n",
    "  storage_context = StorageContext.from_defaults(\n",
    "      vector_store=vector_store, persist_dir=index_dir\n",
    "  )\n",
    "  index = load_index_from_storage(storage_context=storage_context, embed_model=embed_model)\n",
    "  print(\"Using stored index.\")\n",
    "  \n",
    "else:\n",
    "  # Create Index\n",
    "  faiss_index = faiss.IndexFlatL2(384)\n",
    "  vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "  index = VectorStoreIndex(\n",
    "      nodes,\n",
    "      embed_model=embed_model,\n",
    "      storage_context=storage_context,\n",
    "  )\n",
    "\n",
    "  # Save index\n",
    "  index.storage_context.persist(persist_dir=index_dir)\n",
    "  print(f\"Index stored in {index_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"KISSKI_URL\")\n",
    "\n",
    "if not api_key or not base_url:\n",
    "    raise ValueError(\"Missing API key or URL.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "def ask_openai_llm(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": answer_level},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "967468de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Is the sea getting more carbonated?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The question of whether the sea is getting more carbonated is a complex one, but I'll try to provide a concise answer.\n",
       "\n",
       "The key concept here is ocean acidification, which is a consequence of the increasing amount of carbon dioxide (CO2) in the atmosphere due to human activities, such as burning fossil fuels and deforestation. When CO2 dissolves in seawater, it forms carbonic acid, which increases the acidity of the ocean. This is known as ocean acidification.\n",
       "\n",
       "The main driver of ocean acidification is the uptake of CO2 by the ocean, which has increased by about 30% since the Industrial Revolution. This increase in CO2 has led to a decrease in the pH of the ocean by about 0.1 units, which may not seem significant, but it has a profound impact on marine life, particularly organisms with calcium carbonate shells, such as coral, shellfish, and some plankton.\n",
       "\n",
       "The increased acidity of the ocean can lead to a range of consequences, including:\n",
       "\n",
       "1. Reduced growth rates and increased mortality of marine organisms with calcium carbonate shells.\n",
       "2. Changes in the composition and structure of marine ecosystems.\n",
       "3. Impacts on the global carbon cycle, as the ocean acts as a sink for CO2.\n",
       "\n",
       "In the context of small island developing states, ocean acidification can exacerbate the already significant challenges they face, such as erosion, flooding, and loss of livelihoods.\n",
       "\n",
       "So, to answer your question, the sea is indeed getting more acidic, but not in the classical sense of \"carbonation.\" The term \"carbonation\" usually refers to the process of dissolving CO2 in water to create a fizzy or carbonated beverage. In the context of ocean acidification, the term \"acidification\" is more accurate, as it describes the increase in acidity due to the dissolution of CO2 in seawater."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___\n",
      "\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your question (or type 'q'): \").strip()\n",
    "    if query.lower() == 'q':\n",
    "        print(\"Session ended.\")\n",
    "        break\n",
    "\n",
    "    nodes = index.as_retriever().retrieve(query)\n",
    "    context = \"\\n---\\n\".join([n.get_content() for n in nodes])\n",
    "    full_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "\n",
    "    answer = ask_openai_llm(full_prompt)\n",
    "    print(f\"\\nQ:\")\n",
    "    display(Markdown(textwrap.dedent(query)))\n",
    "    print(\"\\nA:\")\n",
    "    display(Markdown(textwrap.dedent(answer)))\n",
    "    print(\"___\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
