{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda795eb",
   "metadata": {},
   "source": [
    "- create .venv\n",
    "- run `pip install -r requirements.txt`\n",
    "- get API key from https://kisski.gwdg.de/leistungen/2-02-llm-service/\n",
    "- create `.env` file with:\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://chat-ai.academiccloud.de/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4743fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n",
      "Generated 326 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Load HTML File/s\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"html\").load_data()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# Chunk with SentenceSplitter\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200) \n",
    "# 1 token = 4 characters\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Generated {len(nodes)} chunks.\") \n",
    "\n",
    "\n",
    "# Embed Chunks with HuggingFace\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b32532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index stored in ./faiss_index\n"
     ]
    }
   ],
   "source": [
    "# Create Index\n",
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(384)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# Save index\n",
    "index.storage_context.persist(persist_dir=\"./faiss_index\")\n",
    "print(\"Index stored in ./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44a03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"KISSKI_URL\")\n",
    "\n",
    "if not api_key or not base_url:\n",
    "    raise ValueError(\"Missing API key or URL.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "def ask_openai_llm(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama-3.1-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "967468de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Where are the best curries in India?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm happy to help, but the context provided doesn't seem to be related to the question about the best curries in India. The context appears to be about the impact of climate change on agriculture in Pacific island regions and the Caribbean.\n",
       "\n",
       "If you could provide more context or information about what you're looking for, I'd be happy to try and assist you. Alternatively, if you'd like to discuss the impact of climate change on agriculture or other related topics, I'd be happy to chat with you about that as well."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___\n",
      "\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your question (or type 'q'): \").strip()\n",
    "    if query.lower() == 'q':\n",
    "        print(\"Session ended.\")\n",
    "        break\n",
    "\n",
    "    nodes = index.as_retriever().retrieve(query)\n",
    "    context = \"\\n---\\n\".join([n.get_content() for n in nodes])\n",
    "    full_prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "\n",
    "    answer = ask_openai_llm(full_prompt)\n",
    "    print(f\"\\nQ:\")\n",
    "    display(Markdown(textwrap.dedent(query)))\n",
    "    print(\"\\nA:\")\n",
    "    display(Markdown(textwrap.dedent(answer)))\n",
    "    print(\"___\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6215ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
