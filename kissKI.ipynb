{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda795eb",
   "metadata": {},
   "source": [
    "- create .venv\n",
    "- run `pip install -r requirements.txt`\n",
    "- get API key from https://kisski.gwdg.de/leistungen/2-02-llm-service/\n",
    "- create `.env` file with:\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://chat-ai.academiccloud.de/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4743fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n",
      "Generated 326 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load HTML File/s\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"html\").load_data()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# 2. Chunk with SentenceSplitter\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200) # 1 token = 4 characters\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Generated {len(nodes)} chunks.\") \n",
    "\n",
    "\n",
    "# 3. Embed Chunks with HuggingFace\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46b32532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index stored in ./faiss_index\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Index\n",
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(384)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# Persist index to disk\n",
    "index.storage_context.persist(persist_dir=\"./faiss_index\")\n",
    "print(\"Index stored in ./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e44a03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define LLM\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"KISSKI_URL\")\n",
    "\n",
    "if not api_key or not base_url:\n",
    "    raise ValueError(\"Missing API key or URL.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "def ask_openai_llm(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama-3.1-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "967468de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "How is this change impacting Native peoples?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Unfortunately, the provided text does not specifically address the impact of climate change on Native peoples. The text primarily focuses on the Pacific Islands, including Samoa, Dominica, and Kiribati, and discusses the role of culture, social capital, and traditional knowledge in adaptation efforts.\n",
       "\n",
       "However, it's worth noting that the text mentions that \"culture is often overlooked in adaptation policies and plans\" (referring to the National Communications of 16 Small Island Developing States). This suggests that the experiences and perspectives of indigenous peoples may be marginalized or ignored in climate change adaptation efforts.\n",
       "\n",
       "If you're looking for information on the impact of climate change on Native peoples or indigenous communities, I'd be happy to try and provide more general information or point you in the direction of additional resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___\n",
      "\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "# 6. Ask a Question\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your question (or type 'q'): \").strip()\n",
    "    if query.lower() == 'q':\n",
    "        print(\"Session ended.\")\n",
    "        break\n",
    "\n",
    "    nodes = index.as_retriever().retrieve(query)\n",
    "    context = \"\\n---\\n\".join([n.get_content() for n in nodes])\n",
    "    full_prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "\n",
    "    answer = ask_openai_llm(full_prompt)\n",
    "    print(f\"\\nQ:\")\n",
    "    display(Markdown(textwrap.dedent(query)))\n",
    "    print(\"\\nA:\")\n",
    "    display(Markdown(textwrap.dedent(answer)))\n",
    "    print(\"___\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6215ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, node in enumerate(nodes[:5]):\n",
    "#     print(f\"\\nChunk {i + 1}\")\n",
    "#     print(f\"Characters: {len(node.get_content())}\")\n",
    "#     print(f\"Preview: {node.get_content()[:500]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
