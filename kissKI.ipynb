{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda795eb",
   "metadata": {},
   "source": [
    ".venv/bin/python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4743fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n",
      "Generated 144 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load HTML File/s\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"html\").load_data()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# 2. Chunk with SentenceSplitter\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=2048, chunk_overlap=200) # 1 token = 4 characters\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Generated {len(nodes)} chunks.\") \n",
    "\n",
    "# 3. Embed Chunks with HuggingFace\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46b32532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index persisted to ./faiss_index\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Index\n",
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(384)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# Persist index to disk\n",
    "index.storage_context.persist(persist_dir=\"./faiss_index\")\n",
    "print(\"Index persisted to ./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e44a03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define LLM\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"KISSKI_URL\")\n",
    "\n",
    "if not api_key or not base_url:\n",
    "    raise ValueError(\"Missing KISSKI_API_KEY or KISSKI_URL in environment.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "def ask_openai_llm(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama-3.1-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "967468de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "what is an island?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "An island is a piece of land that is surrounded by water on all sides. Islands can be small or large, and they can be found in various parts of the world, including oceans, seas, and lakes. In the context of the provided text, islands are specifically mentioned as being small and vulnerable to climate change impacts, such as sea-level rise (SLR) and changes in weather patterns."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___\n",
      "\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "# 6. Ask a Question\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "while True:\n",
    "    query = input(\"üîç Enter your question (or type 'q'): \").strip()\n",
    "    if query.lower() == 'q':\n",
    "        print(\"Session ended.\")\n",
    "        break\n",
    "\n",
    "    nodes = index.as_retriever().retrieve(query)\n",
    "    context = \"\\n---\\n\".join([n.get_content() for n in nodes])\n",
    "    full_prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "\n",
    "    answer = ask_openai_llm(full_prompt)\n",
    "    print(f\"\\nQ:\")\n",
    "    display(Markdown(textwrap.dedent(query)))\n",
    "    print(\"\\nA:\")\n",
    "    display(Markdown(textwrap.dedent(answer)))\n",
    "    print(\"___\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
