{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbc812e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using RAG to Engage with IPCC Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2b8a3",
   "metadata": {},
   "source": [
    "## First Steps\n",
    "- create .venv (I'm using Python 3.12)  \n",
    "- install requirements.txt (either automatically while setting up, or run `pip install -r requirements.txt`)\n",
    "- get API key from:\n",
    "    - https://kisski.gwdg.de/leistungen/2-02-llm-service/ or\n",
    "    - https://console.groq.com/keys  \n",
    "    &rarr; Groq can be used with the OpenAI library with limitations, see https://console.groq.com/keys\n",
    "  - create `.env` file with:\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://chat-ai.academiccloud.de/v1\"  \n",
    "\n",
    "  OR (for Groq):  \n",
    "\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://api.groq.com/openai/v1\"  \n",
    "\n",
    "  (if you change the variable name you'll have to change it in the code too)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a23198",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "![graph.png](graph.png)\n",
    "\n",
    "1. Extract information from official IPCC reports\n",
    "\n",
    "2. Prepare the data for smart search\n",
    "\n",
    "3. Use AI to answer relevant questions\n",
    "\n",
    "4. Log everything for evaluation and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56594649",
   "metadata": {},
   "source": [
    "## Code & Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51752066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fynn/Coding/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import faiss\n",
    "import os\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc59955",
   "metadata": {},
   "source": [
    "- Text and data processing (BeautifulSoup, glob, json, os)\n",
    "\n",
    "- Embedding and search (HuggingFace, FAISS)\n",
    "\n",
    "- Language model connections (OpenAI, LLaMA)\n",
    "\n",
    "- Scoring and evaluation tools (BERTScore, ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf0205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Transform HTML\n",
    "#####\n",
    "\n",
    "input_folder = \"html\"\n",
    "output_file = \"txt/numbered_chunks.txt\"\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# Loop through all HTML files\n",
    "for html_file in glob.glob(os.path.join(input_folder, \"*.html\")):\n",
    "    with open(html_file, encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        # Find all paragraphs with an id\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            pid = p.get(\"id\")\n",
    "            text = p.get_text().strip()\n",
    "            if pid and text:\n",
    "                chunk = f\"[{pid}] {text}\"\n",
    "                all_chunks.append(chunk)\n",
    "\n",
    "# Save all chunks to a text file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(chunk + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b8c0c",
   "metadata": {},
   "source": [
    "This step takes raw, complex IPCC reports (in HTML) and breaks them down into manageable pieces -- one paragraph per line, each with a unique ID.\n",
    "\n",
    "Advantages: \n",
    "\n",
    "- A lot more readable  \n",
    "&rarr; allows users to get answers that can be traced back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e57a2940",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Enums and variables\n",
    "#####\n",
    "\n",
    "\n",
    "ID_prompt = \"\"\"\n",
    "Pass back the full ID of the paragraph(s) from the input file you're taking the information from.\n",
    "\"\"\"\n",
    "\n",
    "class Prompt(Enum):\n",
    "    BASIC = f'{ID_prompt} You are explaining to someone with basic knowledge of the topic.'\n",
    "    ADVANCED = f'{ID_prompt} You are explaining to someone with advanced knowledge of the topic.'\n",
    "\n",
    "class Model(Enum):\n",
    "    LLAMA = 'meta-llama-3.1-8b-instruct'\n",
    "    GEMMA = 'gemma-3-27b-it'\n",
    "    \n",
    "class Embedding(Enum):\n",
    "    MINILM = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    GTR = \"sentence-transformers/gtr-t5-base\"\n",
    "    MPNET = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "\n",
    "EMBED_DIM_MAP = {\n",
    "    Embedding.MINILM: 384,\n",
    "    Embedding.MPNET: 768,\n",
    "    Embedding.GTR: 768\n",
    "}\n",
    "\n",
    "jsonl_filepath = \"eval/log.jsonl\"\n",
    "\n",
    "llm_model = Model[\"LLAMA\"]\n",
    "answer_level = Prompt[\"BASIC\"]\n",
    "embed_model = Embedding[\"MINILM\"]\n",
    "vector_dimensions = EMBED_DIM_MAP[Embedding.MINILM]\n",
    "\n",
    "index_dir = \"./faiss_index\"\n",
    "input_dir = \"./txt\"\n",
    "tokens_per_chunk = 1024\n",
    "chunk_overlap = 200\n",
    "force_rebuild = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321be678",
   "metadata": {},
   "source": [
    "All of these can be changed to suit the source texts better. \n",
    "\n",
    "- Different file paths  \n",
    "\n",
    "- Different models for LLM & Embeddings\n",
    "\n",
    "- Prompt base for the answers can be adjusted (give back paragraph IDs, depth of explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad972f35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Load, chunk, and embed input file\n",
    "#####\n",
    "\n",
    "def make_index(index_dir, embed_model, force_rebuild):\n",
    "\n",
    "    # Embed Chunks with HuggingFace\n",
    "    embedder = HuggingFaceEmbedding(model_name=embed_model)\n",
    "\n",
    "    vector_store = FaissVectorStore.from_persist_dir(index_dir)\n",
    "\n",
    "    faiss_index = vector_store._faiss_index\n",
    "    stored_dim = faiss_index.d\n",
    "\n",
    "    #if:\n",
    "    # - not instructed to rebuild index\n",
    "    # - stored index fits the dimensions required by embedding model\n",
    "    # - index directory exists\n",
    "    # - index directory isn't empty\n",
    "    if (not force_rebuild) and (stored_dim == vector_dimensions) and os.path.exists(index_dir) and os.listdir(index_dir):\n",
    "\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            vector_store=vector_store, persist_dir=index_dir\n",
    "        )\n",
    "        index = load_index_from_storage(storage_context=storage_context, embed_model=embedder)\n",
    "        print(\"Using stored index.\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Load HTML file(s)\n",
    "        documents = SimpleDirectoryReader(input_dir=input_dir).load_data()\n",
    "        print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "        # Chunk with SentenceSplitter (progress bar per doc)\n",
    "        splitter = SentenceSplitter(chunk_size=tokens_per_chunk, chunk_overlap=chunk_overlap)\n",
    "\n",
    "        nodes = []\n",
    "        for doc in documents:\n",
    "            nodes.extend(splitter.get_nodes_from_documents([doc]))\n",
    "\n",
    "        print(f\"Generated {len(nodes)} chunks.\")\n",
    "\n",
    "        # Create Index\n",
    "        faiss_index = faiss.IndexFlatL2(vector_dimensions)\n",
    "        vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "        index = VectorStoreIndex(\n",
    "            nodes,\n",
    "            embed_model=embedder,\n",
    "            storage_context=storage_context,\n",
    "        )\n",
    "\n",
    "        # Save index\n",
    "        index.storage_context.persist(persist_dir=index_dir)\n",
    "        print(f\"Index stored in {index_dir}\")\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18c1fb",
   "metadata": {},
   "source": [
    "Here we create a “searchable memory” of all the report paragraphs, using AI-powered embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3f2534f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# LLM\n",
    "#####\n",
    "\n",
    "def load_llm(llm_model, answer_level):\n",
    "    \n",
    "  load_dotenv()\n",
    "\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "  base_url = os.getenv(\"KISSKI_URL\")\n",
    "\n",
    "  if not api_key or not base_url:\n",
    "      raise ValueError(\"Missing API key or URL.\")\n",
    "\n",
    "  client = OpenAI(\n",
    "      api_key=api_key,\n",
    "      base_url=base_url\n",
    "  )\n",
    "\n",
    "  def ask_openai_llm(prompt: str) -> str:\n",
    "      response = client.chat.completions.create(\n",
    "          model=llm_model,\n",
    "          messages=[\n",
    "              {\"role\": \"system\", \"content\": answer_level},\n",
    "              {\"role\": \"user\", \"content\": prompt}\n",
    "          ]\n",
    "      )\n",
    "      return response.choices[0].message.content\n",
    "  return ask_openai_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b355",
   "metadata": {},
   "source": [
    "This function sets up the language model (“the brain” of the system).\n",
    "\n",
    "It connects to an API hosted by a German computing centre (KISSKI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "925ebf4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Log for Eval\n",
    "#####\n",
    "\n",
    "def log_rag_example(filepath, question, answer, retrieved_context, reference=None):\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_context\": retrieved_context,\n",
    "            \"reference_answer\": reference\n",
    "        }) + \"\\n\")\n",
    "\n",
    "        \n",
    "#####\n",
    "# Query\n",
    "#####\n",
    "\n",
    "def ask_question(index, ask_openai_llm):\n",
    "\n",
    "\n",
    "  while True:\n",
    "      query = input(\"Enter your question (or type 'q'): \").strip()\n",
    "      if query.lower() == 'q':\n",
    "          print(\"Session ended.\")\n",
    "          break\n",
    "\n",
    "      nodes = index.as_retriever().retrieve(query)\n",
    "      context = \"\\n---\\n\".join([n.get_content() for n in nodes])\n",
    "\n",
    "      full_prompt = f\"\"\"\n",
    "  Context:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {query}\"\"\"\n",
    "\n",
    "      answer = ask_openai_llm(full_prompt)\n",
    "      print(f\"\\nQ:\")\n",
    "      display(Markdown(textwrap.dedent(query)))\n",
    "      print(\"\\nA:\")\n",
    "      display(Markdown(textwrap.dedent(answer)))\n",
    "      print(\"___\\n\")\n",
    "      \n",
    "      answer = str(answer)\n",
    "      log_rag_example(jsonl_filepath, query, answer, context, reference=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bf057",
   "metadata": {},
   "source": [
    "\n",
    "This is where the queries for user questions are put together from the default prompt and user input.\n",
    "\n",
    "Aditionally, every question and answer -- plus all supporting context -- are logged for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0923a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stored index.\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Starting point\n",
    "#####\n",
    "\n",
    "index = make_index(index_dir, embed_model.value, force_rebuild)\n",
    "ask_openai_llm = load_llm(llm_model.value, answer_level.value)\n",
    "ask_question(index, ask_openai_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727ffe6",
   "metadata": {},
   "source": [
    "\n",
    "This runs the main pipeline:\n",
    "\n",
    "1. Builds or loads the search index\n",
    "\n",
    "2. Loads the language model\n",
    "\n",
    "3. Lets you ask a question and get an answer, with clear sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb03e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ROUGE...\n",
      "Evaluating Cosine Similarity...\n",
      "\n",
      "=== RAG EVALUATION RESULTS ===\n",
      "BERTScore:\n",
      "  precision: 0.6850\n",
      "  recall: 0.8348\n",
      "  f1: 0.7525\n",
      "ROUGE:\n",
      "  rouge1: 0.0000\n",
      "  rouge2: 0.0000\n",
      "  rougeL: 0.0000\n",
      "Cosine similarity:\n",
      "  cosine_similarity: -0.0556\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Evaluation\n",
    "#####\n",
    "\n",
    "def load_examples(jsonl_path):\n",
    "    questions, generated, references = [], [], []\n",
    "    with open(jsonl_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            questions.append(ex.get(\"question\", \"\"))\n",
    "            generated.append(ex.get(\"generated_answer\", \"\"))\n",
    "            references.append(ex.get(\"reference_answer\", \"\"))  # empty string if missing\n",
    "    return questions, generated, references\n",
    "def load_examples(jsonl_path):\n",
    "    questions, generated, references = [], [], []\n",
    "    with open(jsonl_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            questions.append(ex.get(\"question\", \"\"))\n",
    "            generated.append(ex.get(\"generated_answer\", \"\"))\n",
    "            ref = ex.get(\"reference_answer\")\n",
    "            references.append(\"\\n\" if ref is None else ref)\n",
    "    return questions, generated, references\n",
    "\n",
    "def evaluate_bertscore(candidates, references, lang=\"en\"):\n",
    "    P, R, F1 = bert_score(candidates, references, lang=lang)\n",
    "    return {\n",
    "        \"precision\": float(P.mean()),\n",
    "        \"recall\": float(R.mean()),\n",
    "        \"f1\": float(F1.mean())\n",
    "    }\n",
    "\n",
    "def evaluate_rouge(candidates, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = [scorer.score(ref, cand) for ref, cand in zip(references, candidates)]\n",
    "    avg_scores = {}\n",
    "    for key in results[0]:\n",
    "        avg_scores[key] = np.mean([r[key].fmeasure for r in results])\n",
    "    return avg_scores\n",
    "\n",
    "def evaluate_cosine(candidates, references, model_name=\"all-mpnet-base-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb_refs = model.encode(references, convert_to_tensor=True)\n",
    "    emb_cands = model.encode(candidates, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(emb_cands, emb_refs)\n",
    "    mean_sim = float(scores.diag().mean())\n",
    "    return {\"cosine_similarity\": mean_sim}\n",
    "\n",
    "def eval(path):\n",
    "    questions, generated, references = load_examples(path)\n",
    "\n",
    "    # Optionally, filter empty references if your gold data is patchy\n",
    "    filtered_gen, filtered_ref = [], []\n",
    "    for g, r in zip(generated, references):\n",
    "        if r.strip():  # has reference\n",
    "            filtered_gen.append(g)\n",
    "            filtered_ref.append(r)\n",
    "    if not filtered_ref:\n",
    "        print(\"No reference answers found in data! Populate 'reference_answer' for proper eval.\")\n",
    "        return\n",
    "\n",
    "    print(\"Evaluating BERTScore...\")\n",
    "    bert = evaluate_bertscore(filtered_gen, filtered_ref)\n",
    "    print(\"Evaluating ROUGE...\")\n",
    "    rouge = evaluate_rouge(filtered_gen, filtered_ref)\n",
    "    print(\"Evaluating Cosine Similarity...\")\n",
    "    cosine = evaluate_cosine(filtered_gen, filtered_ref)\n",
    "\n",
    "    print(\"\\n=== RAG EVALUATION RESULTS ===\")\n",
    "    print(\"BERTScore:\")\n",
    "    for k, v in bert.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"ROUGE:\")\n",
    "    for k, v in rouge.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"Cosine similarity:\")\n",
    "    for k, v in cosine.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "eval(jsonl_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a197654",
   "metadata": {},
   "source": [
    "Evaluates all previously stored Q&As for BERTScore, Rouge, and Cosine Similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
