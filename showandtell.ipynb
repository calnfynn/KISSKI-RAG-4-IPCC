{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbc812e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using RAG to Engage with IPCC Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2b8a3",
   "metadata": {},
   "source": [
    "## First Steps\n",
    "- create .venv (I'm using Python 3.12)  \n",
    "- install requirements.txt (either automatically while setting up, or run `pip install -r requirements.txt`)\n",
    "- get API key from:\n",
    "    - https://kisski.gwdg.de/leistungen/2-02-llm-service/ or\n",
    "    - https://console.groq.com/keys  \n",
    "    &rarr; Groq can be used with the OpenAI library with limitations, see https://console.groq.com/keys\n",
    "  - create `.env` file with:\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://chat-ai.academiccloud.de/v1\"  \n",
    "\n",
    "  OR (for Groq):  \n",
    "\n",
    "  > OPENAI_API_KEY = \"YOUR-API-KEY\"  \n",
    "  > KISSKI_URL = \"https://api.groq.com/openai/v1\"  \n",
    "\n",
    "  (if you change the variable names you'll have to change it in the code too)\n",
    "\n",
    "Note about the models:  \n",
    "  - I use *meta-llama-3.1-8b-instruct* via KISSKI  \n",
    "  - Groq offers *llama-3.1-8b-instant*, *llama-3.3-70b-versatile*, and *meta-llama/llama-guard-4-12b*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a23198",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "![graph.png](graph.png)\n",
    "\n",
    "1. Extract information from official IPCC reports\n",
    "\n",
    "2. Prepare the data for smart search\n",
    "\n",
    "3. Use AI to answer relevant questions\n",
    "\n",
    "4. Log everything for evaluation and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56594649",
   "metadata": {},
   "source": [
    "## Code & Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51752066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-\n",
      "[nltk_data]     packages\\llama_index\\core\\_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "c:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import faiss\n",
    "import os\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc59955",
   "metadata": {},
   "source": [
    "- Text and data processing (BeautifulSoup, glob, json, os)\n",
    "\n",
    "- Embedding and search (HuggingFace, FAISS)\n",
    "\n",
    "- Language model connections (OpenAI, LLaMA)\n",
    "\n",
    "- Scoring and evaluation tools (BERTScore, ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf0205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Transform HTML\n",
    "#####\n",
    "\n",
    "input_folder = \"html\"\n",
    "output_file = \"txt/numbered_chunks.txt\"\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# Loop through all HTML files\n",
    "for html_file in glob.glob(os.path.join(input_folder, \"*.html\")):\n",
    "    with open(html_file, encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        # Find all paragraphs with an id\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            pid = p.get(\"id\")\n",
    "            text = p.get_text().strip()\n",
    "            if pid and text:\n",
    "                chunk = f\"[{pid}] {text}\"\n",
    "                all_chunks.append(chunk)\n",
    "\n",
    "# Save all chunks to a text file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(chunk + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b8c0c",
   "metadata": {},
   "source": [
    "This step takes raw, complex IPCC reports (in HTML) and breaks them down into manageable pieces -- one paragraph per line, each with a unique ID.\n",
    "\n",
    "Advantages: \n",
    "\n",
    "- A lot more readable  \n",
    "&rarr; allows users to get answers that can be traced back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57a2940",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Enums and variables\n",
    "#####\n",
    "\n",
    "\n",
    "ID_prompt = \"\"\"\n",
    "Pass back the full ID of the paragraph(s) from the input file you're taking the information from.\n",
    "\"\"\"\n",
    "\n",
    "class Prompt(Enum):\n",
    "    BASIC = f'{ID_prompt} You are explaining to someone with basic knowledge of the topic.'\n",
    "    ADVANCED = f'{ID_prompt} You are explaining to someone with advanced knowledge of the topic.'\n",
    "\n",
    "class Model(Enum):\n",
    "    LLAMA = 'meta-llama-3.1-8b-instruct'\n",
    "    GEMMA = 'gemma-3-27b-it'\n",
    "    \n",
    "class Embedding(Enum):\n",
    "    MINILM = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    GTR = \"sentence-transformers/gtr-t5-base\"\n",
    "    MPNET = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "\n",
    "EMBED_DIM_MAP = {\n",
    "    Embedding.MINILM: 384,\n",
    "    Embedding.MPNET: 768,\n",
    "    Embedding.GTR: 768\n",
    "}\n",
    "\n",
    "jsonl_filepath = \"eval/log.jsonl\"\n",
    "\n",
    "llm_model = Model[\"LLAMA\"]\n",
    "answer_level = Prompt[\"BASIC\"]\n",
    "embed_model = Embedding[\"MINILM\"]\n",
    "vector_dimensions = EMBED_DIM_MAP[Embedding.MINILM]\n",
    "\n",
    "index_dir = \"./faiss_index\"\n",
    "input_dir = \"./txt\"\n",
    "tokens_per_chunk = 1024\n",
    "chunk_overlap = 200\n",
    "force_rebuild = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321be678",
   "metadata": {},
   "source": [
    "All of these can be changed to suit the source texts better. \n",
    "\n",
    "- Different file paths  \n",
    "\n",
    "- Different models for LLM & Embeddings\n",
    "\n",
    "- Prompt base for the answers can be adjusted (give back paragraph IDs, depth of explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad972f35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Load, chunk, and embed input file\n",
    "#####\n",
    "\n",
    "def make_index(index_dir, embed_model, force_rebuild):\n",
    "\n",
    "    # Embed Chunks with HuggingFace\n",
    "    embedder = HuggingFaceEmbedding(model_name=embed_model)\n",
    "\n",
    "    vector_store = FaissVectorStore.from_persist_dir(index_dir)\n",
    "\n",
    "    faiss_index = vector_store._faiss_index\n",
    "    stored_dim = faiss_index.d\n",
    "\n",
    "    #if:\n",
    "    # - not instructed to rebuild index\n",
    "    # - stored index fits the dimensions required by embedding model\n",
    "    # - index directory exists\n",
    "    # - index directory isn't empty\n",
    "    if (not force_rebuild) and (stored_dim == vector_dimensions) and os.path.exists(index_dir) and os.listdir(index_dir):\n",
    "\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            vector_store=vector_store, persist_dir=index_dir\n",
    "        )\n",
    "        index = load_index_from_storage(storage_context=storage_context, embed_model=embedder)\n",
    "        print(\"Using stored index.\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Load HTML file(s)\n",
    "        documents = SimpleDirectoryReader(input_dir=input_dir).load_data()\n",
    "        print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "        # Chunk with SentenceSplitter (progress bar per doc)\n",
    "        splitter = SentenceSplitter(chunk_size=tokens_per_chunk, chunk_overlap=chunk_overlap)\n",
    "\n",
    "        nodes = []\n",
    "        for doc in documents:\n",
    "            nodes.extend(splitter.get_nodes_from_documents([doc]))\n",
    "\n",
    "        print(f\"Generated {len(nodes)} chunks.\")\n",
    "\n",
    "        # Create Index\n",
    "        faiss_index = faiss.IndexFlatL2(vector_dimensions)\n",
    "        vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "        index = VectorStoreIndex(\n",
    "            nodes,\n",
    "            embed_model=embedder,\n",
    "            storage_context=storage_context,\n",
    "        )\n",
    "\n",
    "        # Save index\n",
    "        index.storage_context.persist(persist_dir=index_dir)\n",
    "        print(f\"Index stored in {index_dir}\")\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18c1fb",
   "metadata": {},
   "source": [
    "Here we create a “searchable memory” of all the report paragraphs, using AI-powered embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f2534f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# LLM\n",
    "#####\n",
    "\n",
    "def load_llm(llm_model, answer_level):\n",
    "    \n",
    "  load_dotenv()\n",
    "\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "  base_url = os.getenv(\"KISSKI_URL\")\n",
    "\n",
    "  if not api_key or not base_url:\n",
    "      raise ValueError(\"Missing API key or URL.\")\n",
    "\n",
    "  client = OpenAI(\n",
    "      api_key=api_key,\n",
    "      base_url=base_url\n",
    "  )\n",
    "\n",
    "  def ask_openai_llm(prompt: str) -> str:\n",
    "      response = client.chat.completions.create(\n",
    "          model=llm_model,\n",
    "          messages=[\n",
    "              {\"role\": \"system\", \"content\": answer_level},\n",
    "              {\"role\": \"user\", \"content\": prompt}\n",
    "          ]\n",
    "      )\n",
    "      return response.choices[0].message.content\n",
    "  return ask_openai_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b355",
   "metadata": {},
   "source": [
    "This function sets up the language model (“the brain” of the system).\n",
    "\n",
    "It connects to an API hosted by a German computing centre (KISSKI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925ebf4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Log for Eval\n",
    "#####\n",
    "\n",
    "def log_rag_example(filepath, question, answer, retrieved_context, reference=None):\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_context\": retrieved_context,\n",
    "            \"reference_answer\": reference\n",
    "        }) + \"\\n\")\n",
    "\n",
    "        \n",
    "#####\n",
    "# Query\n",
    "#####\n",
    "\n",
    "def ask_question(index, ask_openai_llm):\n",
    "\n",
    "\n",
    "  while True:\n",
    "      query = input(\"Enter your question (or type 'q'): \").strip()\n",
    "      if query.lower() == 'q':\n",
    "          print(\"Session ended.\")\n",
    "          break\n",
    "\n",
    "      nodes = index.as_retriever().retrieve(query)\n",
    "      context = \"\\n---\\n\".join([n.get_content() for n in nodes])\n",
    "\n",
    "      full_prompt = f\"\"\"\n",
    "  Context:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {query}\"\"\"\n",
    "\n",
    "      answer = ask_openai_llm(full_prompt)\n",
    "      print(f\"\\nQ:\")\n",
    "      display(Markdown(textwrap.dedent(query)))\n",
    "      print(\"\\nA:\")\n",
    "      display(Markdown(textwrap.dedent(answer)))\n",
    "      print(\"___\\n\")\n",
    "      \n",
    "      answer = str(answer)\n",
    "      log_rag_example(jsonl_filepath, query, answer, context, reference=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bf057",
   "metadata": {},
   "source": [
    "\n",
    "This is where the queries for user questions are put together from the default prompt and user input.\n",
    "\n",
    "Aditionally, every question and answer -- plus all supporting context -- are logged for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0923a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Starting point\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m index = \u001b[43mmake_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_rebuild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m ask_openai_llm = load_llm(llm_model.value, answer_level.value)\n\u001b[32m      7\u001b[39m ask_question(index, ask_openai_llm)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mmake_index\u001b[39m\u001b[34m(index_dir, embed_model, force_rebuild)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_index\u001b[39m(index_dir, embed_model, force_rebuild):\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Embed Chunks with HuggingFace\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     embedder = \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     vector_store = FaissVectorStore.from_persist_dir(index_dir)\n\u001b[32m     12\u001b[39m     faiss_index = vector_store._faiss_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\llama_index\\embeddings\\huggingface\\base.py:153\u001b[39m, in \u001b[36mHuggingFaceEmbedding.__init__\u001b[39m\u001b[34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager, parallel_process, target_devices, **model_kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `model_name` argument must be provided.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_instruction\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_query_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_instruction\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_text_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_length:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mself\u001b[39m._model.max_seq_length = max_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:547\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m] = kwargs_orig[\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    546\u001b[39m has_remote_code = \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m has_local_code = \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m trust_remote_code = resolve_trust_remote_code(\n\u001b[32m    549\u001b[39m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[32m    550\u001b[39m )\n\u001b[32m    551\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m] = trust_remote_code\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:791\u001b[39m, in \u001b[36m_LazyAutoMapping.keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m     mapping_keys = \u001b[43m[\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config_mapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._extra_content.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:792\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    791\u001b[39m     mapping_keys = [\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items()\n\u001b[32m    794\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping.keys()\n\u001b[32m    795\u001b[39m     ]\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._extra_content.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:787\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    785\u001b[39m module_name = model_type_to_module_name(model_type)\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers.models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(\u001b[38;5;28mself\u001b[39m._modules[module_name], attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\models\\bridgetower\\__init__.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     29\u001b[39m _file = \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\u001b[34m__name__\u001b[39m, _file, \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m, module_spec=__spec__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2597\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2573\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m()\n\u001b[32m   2574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2575\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2595\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2596\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2598\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2600\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2302\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2248\u001b[39m \u001b[33;03mThis method takes the path to a file/a folder and returns the import structure.\u001b[39;00m\n\u001b[32m   2249\u001b[39m \u001b[33;03mIf a file is given, it will return the import structure of the parent folder.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2298\u001b[39m \u001b[33;03m}\u001b[39;00m\n\u001b[32m   2299\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2300\u001b[39m import_structure = {}\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2303\u001b[39m     module_path = os.path.dirname(module_path)\n\u001b[32m   2305\u001b[39m directory = module_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:30\u001b[39m, in \u001b[36misfile\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Starting point\n",
    "#####\n",
    "\n",
    "index = make_index(index_dir, embed_model.value, force_rebuild)\n",
    "ask_openai_llm = load_llm(llm_model.value, answer_level.value)\n",
    "ask_question(index, ask_openai_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727ffe6",
   "metadata": {},
   "source": [
    "\n",
    "This runs the main pipeline:\n",
    "\n",
    "1. Builds or loads the search index\n",
    "\n",
    "2. Loads the language model\n",
    "\n",
    "3. Lets you ask a question and get an answer, with clear sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RahrA\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ROUGE...\n",
      "Evaluating Cosine Similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RahrA\\Desktop\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RahrA\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG EVALUATION RESULTS ===\n",
      "BERTScore:\n",
      "  precision: 0.6850\n",
      "  recall: 0.8348\n",
      "  f1: 0.7525\n",
      "ROUGE:\n",
      "  rouge1: 0.0000\n",
      "  rouge2: 0.0000\n",
      "  rougeL: 0.0000\n",
      "Cosine similarity:\n",
      "  cosine_similarity: -0.0556\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Evaluation\n",
    "#####\n",
    "\n",
    "def load_examples(jsonl_path):\n",
    "    questions, generated, references = [], [], []\n",
    "    with open(jsonl_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            questions.append(ex.get(\"question\", \"\"))\n",
    "            generated.append(ex.get(\"generated_answer\", \"\"))\n",
    "            references.append(ex.get(\"reference_answer\", \"\"))  # empty string if missing\n",
    "    return questions, generated, references\n",
    "def load_examples(jsonl_path):\n",
    "    questions, generated, references = [], [], []\n",
    "    with open(jsonl_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            questions.append(ex.get(\"question\", \"\"))\n",
    "            generated.append(ex.get(\"generated_answer\", \"\"))\n",
    "            ref = ex.get(\"reference_answer\")\n",
    "            references.append(\"\\n\" if ref is None else ref)\n",
    "    return questions, generated, references\n",
    "\n",
    "def evaluate_bertscore(candidates, references, lang=\"en\"):\n",
    "    P, R, F1 = bert_score(candidates, references, lang=lang)\n",
    "    return {\n",
    "        \"precision\": float(P.mean()),\n",
    "        \"recall\": float(R.mean()),\n",
    "        \"f1\": float(F1.mean())\n",
    "    }\n",
    "\n",
    "def evaluate_rouge(candidates, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    results = [scorer.score(ref, cand) for ref, cand in zip(references, candidates)]\n",
    "    avg_scores = {}\n",
    "    for key in results[0]:\n",
    "        avg_scores[key] = np.mean([r[key].fmeasure for r in results])\n",
    "    return avg_scores\n",
    "\n",
    "def evaluate_cosine(candidates, references, model_name=\"all-mpnet-base-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb_refs = model.encode(references, convert_to_tensor=True)\n",
    "    emb_cands = model.encode(candidates, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(emb_cands, emb_refs)\n",
    "    mean_sim = float(scores.diag().mean())\n",
    "    return {\"cosine_similarity\": mean_sim}\n",
    "\n",
    "def eval(path):\n",
    "    questions, generated, references = load_examples(path)\n",
    "\n",
    "    # Optionally, filter empty references if your gold data is patchy\n",
    "    filtered_gen, filtered_ref = [], []\n",
    "    for g, r in zip(generated, references):\n",
    "        if r.strip():  # has reference\n",
    "            filtered_gen.append(g)\n",
    "            filtered_ref.append(r)\n",
    "    if not filtered_ref:\n",
    "        print(\"No reference answers found in data! Populate 'reference_answer' for proper eval.\")\n",
    "        return\n",
    "\n",
    "    print(\"Evaluating BERTScore...\")\n",
    "    bert = evaluate_bertscore(filtered_gen, filtered_ref)\n",
    "    print(\"Evaluating ROUGE...\")\n",
    "    rouge = evaluate_rouge(filtered_gen, filtered_ref)\n",
    "    print(\"Evaluating Cosine Similarity...\")\n",
    "    cosine = evaluate_cosine(filtered_gen, filtered_ref)\n",
    "\n",
    "    print(\"\\n=== RAG EVALUATION RESULTS ===\")\n",
    "    print(\"BERTScore:\")\n",
    "    for k, v in bert.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"ROUGE:\")\n",
    "    for k, v in rouge.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"Cosine similarity:\")\n",
    "    for k, v in cosine.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "eval(jsonl_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a197654",
   "metadata": {},
   "source": [
    "Evaluates all previously stored Q&As for BERTScore, Rouge, and Cosine Similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
